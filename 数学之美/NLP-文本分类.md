## 对象

### 数据

大数据采集

1. 短文本
    1. 句子
    2. 标题
    3. 商品评论
2. 长文本
    1. 文章

### 标签类别

一般是人工划分（二分类或多分类）

1. 政治、体育、军事
2. 正能量、负能量
3. 好评、差评

## 流程

1. 获取训练集
    1. 爬虫
    2. 离线数据
    3. 流计算
2. 特征工程
    1. 预处理
        1. 清洗
            1. 非文本数据：HTML标签、URL地址等
            2. 长串数字或字母：看特定的业务场景而定
                1. 一般可去除：手机号、用户名ID等内容
                2. 转换为归一化的特征：
                    1. 是否出现长串的布尔值特征：HAS_LONG_DIGITAL
                    2. 按长度归一的特征：DIGITAL_LEN_10
            3. 无意义文本：按业务场景定哪些文本是不需要被模型学习的
            4. 变形词识别：特殊符号替换、同音近型替换、简繁替换等
                1. 通过映射表
                2. 拼音首字母来鉴别同音替换
                3. 用``Word2vec``词向量来对比变形词与上下文的语意关联度，从而识别出该词是否经过了变形
        2. 去停用词和标点符号：代词、介词、连接词
            1. 需根据不同情景，做针对性调整
    2. 特征提取
        1. 分词
            1. 中文
                1. 基于字符串匹配
                    1. 这是一种基于词典的中文分词，核心是首先建立统一的词典表，当需要对一个句子进行分词时，首先将句子拆分成多个部分，将每一个部分与字典一一对应，如果该词语在词典中，分词成功，否则继续拆分匹配直到成功
                    2. 优点是速度快，时间复杂度可以保持在O（n）,实现简单，效果尚可；但对歧义和未登录词处理效果不佳
                2. 基于理解的分词：句法、语义分析
                3. 基于统计的分词
                    1. 统计学认为分词是一个概率最大化问题，即拆分句子，基于语料库，统计相邻的字组成的词语出现的概率，相邻的词出现的次数多，就出现的概率大，按照概率值进行分词，所以一个完整的语料库很重要
                    2. N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）
            2. 英文：空格和标点符号来分词
    3. 特征组合与分桶标记(下文采用的文本例句：“小明去学校上自习”)
        1. 特征组合：将文本按一定长度阈值划分为两类（长文本的长度大于20，否则为短文本），如，“长文本_小明”、“短文本_学校”，通过组合特征使得模型从非线性的角度进行分类
        2. 分桶标记：采用不同的特征方法，如“skipgram:小明_学校”、“wordseg:小明”
    4. 特征选择
        1. 卡方检验
        2. 信息增益
    5. 文本表示
        1. 词袋法：忽略其词序和语法，句法，将文本仅仅看做是一个词集合。若词集合共有NN个词，每个文本表示为一个NN维向量，元素为0/1，表示该文本是否包含对应的词。( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0)
        2. n-gram词袋：与词袋模型类似，考虑了局部的顺序信息，但是向量的维度过大，基本不采用。如果词集合大小为N，则bi-gram的单词总数为N2向量空间模型。
        3. skip-gram词袋：有别于``word2vec``中获得词向量的``skip-gram``模型，这里中的``skip-gram``模型表示的是一种衍生自``n-gram``模型的语言模型。对于例句“小明去学校上自习”，常用的``1-skip-bi-gram``得到的特征为{“小明_学校”，“去_上”，“学校_自习”}。一般情况下，``skip-gram``可以做为``n-gram``的补充，从而提取一些可能遗漏的有效特征。
        3. 向量空间模型：以词袋模型为基础，向量空间模型通过特征选择降低维度，通过特征权重计算增加稠密性。
        4. 特征权重计算
            1. 布尔权重：若出现则为1，否则为0，也就是词袋模型
            2. TFIDF则是基于词频来进行定义权重
            3. 基于熵的则是将出现在同一文档的特征赋予较高的权重
3. 构造分类器（模型）
    1. 传统机器学习
        1. NavieBayes
        2. KNN
        3. 决策树
        4. SVM
        5. GBDT / XGBOOST
    2. 深度学习
        1. FastText
        2. TextCNN
        3. TextRNN
        4. RCNN
        5. BERT
    3. 混合（深度学习）：深度学习模型之间的混合，如cnn+lstm、lstm+attention
4. 分类

## 评估

1. accuracy和error rate
2. precision/recall/F-measure
3. exact match（EM）
4. mean reciprocal rank（MRR）

## 应用

1. 垃圾邮件/广告过滤/反黄识别
2. 根据标题为图文视频打标签
3. 根据用户阅读内容建立画像标签
4. 电商商品评论分析
5. 自动问答（QA）
6. 情感分类
7. 自然语言推理
8. 阅读理解

## 问题

### 基于规则（句法、语义分析）

1. 人力：人为借助经验定义规则
2. 维护性
    1. 规则变更需要人工重新总结
    2. 规则太多容易产生冲突
3. 扩展性：依赖业务场景，难以迁移

### 传统机器学习

1. 人工指定特征，特征工程耦合业务场景，难以泛化到其他场景

### 深度学习

1. 复杂的场景下，缺乏数据集
2. 对知识进行建模：需要对文本信息中的知识进行建模，如构建知识库、知识图谱，并基于这些知识进行分析及推理
3. 可解释性不强
4. 更小、更高效的模型：针对BERT来说
5. 小样本学习：当前深度学习的模型太依赖大量的标注数据

## 参考

1. [文本分类算法综述](https://zhuanlan.zhihu.com/p/76003775)
2. [深度学习文本分类综述](https://zhuanlan.zhihu.com/p/129271523)
3. [中文文本分类：你需要了解的10项关键内容](https://www.jiqizhixin.com/articles/2018-10-29-10)